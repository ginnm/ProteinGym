Model_rank,Model_name,Model type,Average_MCC,Bootstrap_standard_error_MCC,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Human,Other Eukaryote,Prokaryote,Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,TranceptEVE L,Hybrid model,0.373,0.0,0.372,0.362,0.406,0.357,0.405,0.408,0.348,0.335,0.276,0.335,0.31,0.318,TranceptEVE Large model (Tranception Large & retrieved EVE model),"<a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a>"
2,GEMME,Alignment-based model,0.366,0.007,0.347,0.36,0.404,0.34,0.39,0.389,0.37,0.331,0.287,0.289,0.281,0.3,GEMME model,"<a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, É., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a>"
3,EVE (ensemble),Alignment-based model,0.356,0.004,0.339,0.348,0.396,0.339,0.382,0.395,0.332,0.308,0.248,0.282,0.262,0.278,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
4,VESPA,Protein language model,0.356,0.008,0.34,0.336,0.429,0.344,0.39,0.42,0.298,0.321,0.213,0.121,0.12,0.112,VESPA model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
5,Tranception L,Hybrid model,0.355,0.004,0.369,0.345,0.369,0.343,0.401,0.376,0.329,0.328,0.311,0.345,0.325,0.339,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
6,EVE (single),Alignment-based model,0.351,0.004,0.33,0.344,0.391,0.331,0.376,0.389,0.331,0.305,0.244,0.295,0.258,0.283,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
7,MSA Transformer (ensemble),Hybrid model,0.346,0.009,0.321,0.337,0.397,0.33,0.388,0.39,0.307,0.31,0.228,0.276,0.245,0.242,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
8,Tranception M,Hybrid model,0.342,0.007,0.345,0.339,0.346,0.34,0.377,0.345,0.324,0.323,0.262,0.229,0.24,0.293,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
9,MSA Transformer (single),Hybrid model,0.336,0.01,0.31,0.329,0.383,0.316,0.389,0.387,0.292,0.31,0.238,0.295,0.244,0.24,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
10,DeepSequence (ensemble),Alignment-based model,0.336,0.007,0.322,0.318,0.404,0.335,0.381,0.393,0.264,0.294,0.247,0.312,0.285,0.294,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
11,Tranception S,Hybrid model,0.332,0.007,0.344,0.323,0.346,0.323,0.37,0.339,0.319,0.313,0.272,0.233,0.23,0.292,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
12,EVmutation,Alignment-based model,0.33,0.004,0.322,0.322,0.359,0.321,0.354,0.375,0.289,0.279,0.218,0.248,0.235,0.246,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
13,Progen2 (ensemble),Protein language model,0.328,0.009,0.296,0.329,0.36,0.327,0.362,0.364,0.281,0.318,0.292,0.229,0.196,0.204,Ensemble of the 5 Progen2 models,"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
14,VESPAl,Protein language model,0.327,0.009,0.322,0.304,0.396,0.309,0.365,0.384,0.281,0.292,0.203,0.083,0.113,0.126,VESPAl model,"<a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a>"
15,DeepSequence (single),Alignment-based model,0.322,0.007,0.318,0.305,0.374,0.325,0.366,0.37,0.251,0.282,0.249,0.257,0.273,0.293,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
16,ESM-1v (ensemble),Protein language model,0.321,0.016,0.293,0.297,0.418,0.348,0.341,0.398,0.203,0.301,0.264,0.181,0.138,0.186,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
17,Tranception L no retrieval,Protein language model,0.318,0.008,0.303,0.315,0.34,0.297,0.348,0.353,0.298,0.292,0.259,0.295,0.295,0.334,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a>"
18,Progen2 XL,Protein language model,0.316,0.008,0.286,0.312,0.356,0.276,0.365,0.385,0.284,0.303,0.283,0.316,0.272,0.267,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
19,Wavenet,Alignment-based model,0.311,0.009,0.252,0.312,0.366,0.315,0.351,0.366,0.233,0.284,0.238,0.196,0.208,0.169,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
20,RITA (ensemble),Protein language model,0.309,0.012,0.257,0.323,0.321,0.307,0.303,0.308,0.315,0.308,0.215,0.147,0.143,0.159,Ensemble of the 4 RITA models,"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
21,Progen2 M,Protein language model,0.308,0.011,0.263,0.312,0.341,0.31,0.321,0.348,0.262,0.304,0.241,0.146,0.139,0.161,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
22,Progen2 L,Protein language model,0.307,0.01,0.291,0.303,0.332,0.309,0.351,0.342,0.248,0.3,0.287,0.264,0.235,0.224,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
23,Progen2 Base,Protein language model,0.305,0.011,0.281,0.305,0.327,0.317,0.333,0.334,0.245,0.303,0.234,0.152,0.16,0.168,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
24,Site-Independent,Alignment-based model,0.304,0.009,0.358,0.303,0.255,0.308,0.331,0.272,0.314,0.283,0.297,0.23,0.244,0.283,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
25,RITA XL,Protein language model,0.301,0.011,0.244,0.312,0.325,0.289,0.298,0.32,0.302,0.297,0.217,0.158,0.144,0.167,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
26,ESM-1v (single),Protein language model,0.3,0.016,0.257,0.282,0.391,0.327,0.323,0.379,0.177,0.283,0.237,0.17,0.129,0.174,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
27,RITA L,Protein language model,0.299,0.012,0.254,0.314,0.298,0.303,0.307,0.279,0.305,0.296,0.199,0.146,0.146,0.17,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
28,RITA M,Protein language model,0.293,0.012,0.258,0.307,0.287,0.292,0.297,0.28,0.305,0.295,0.208,0.124,0.14,0.158,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
29,ESM-1b,Protein language model,0.286,0.015,0.276,0.26,0.371,0.322,0.343,0.383,0.119,0.251,0.202,0.067,0.084,0.131,ESM-1b (w/ Brandes et al. extensions),"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a>"
30,Unirep evotuned,Hybrid model,0.277,0.01,0.26,0.273,0.308,0.276,0.27,0.294,0.268,0.261,0.264,0.259,0.229,0.248,Unirep model w/ evotuning,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
31,Progen2 S,Protein language model,0.269,0.015,0.237,0.273,0.291,0.297,0.284,0.284,0.21,0.273,0.241,0.114,0.134,0.128,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
32,RITA S,Protein language model,0.251,0.014,0.197,0.273,0.242,0.247,0.235,0.23,0.285,0.248,0.222,0.118,0.115,0.116,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
33,ProtGPT2,Protein language model,0.149,0.015,0.143,0.151,0.147,0.188,0.123,0.145,0.11,0.153,0.159,0.061,0.031,0.061,ProtGPT2 model,"<a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a>"
34,Unirep,Protein language model,0.127,0.019,0.173,0.114,0.12,0.205,0.162,0.118,0.007,0.133,0.203,0.099,0.106,0.134,Unirep model,"<a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a>"
